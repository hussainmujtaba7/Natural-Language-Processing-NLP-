{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# here go all the imports\n!pip install seqeval\n!pip install torchtext==0.6.0\nimport csv\nfrom pprint import pprint\nfrom collections import Counter,OrderedDict\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchtext.vocab import Vocab\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom typing import Dict, Iterator, List, Union, Optional\nfrom sklearn.metrics import f1_score \nfrom seqeval.metrics import f1_score as f1\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nSEED = 36\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ndevice='cuda' ","metadata":{"id":"iMhpG_DSQEhG","outputId":"ab329a6c-f132-4970-d811-f439434dd406","execution":{"iopub.status.busy":"2022-04-19T20:15:11.423954Z","iopub.execute_input":"2022-04-19T20:15:11.424226Z","iopub.status.idle":"2022-04-19T20:15:34.695553Z","shell.execute_reply.started":"2022-04-19T20:15:11.424197Z","shell.execute_reply":"2022-04-19T20:15:34.694762Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class NerDataset(Dataset):\n  def __init__(self, \n                  input_file:str, \n                  device=\"cuda\"\n                  ):\n    super().__init__()\n    self.data_file=input_file\n    self.device = device\n    self.data=self.data_load()\n    self.encoded_data = None\n    \n  def data_load(self):\n    final_list=[]\n    wordTag={}\n    temp_word_list=[]\n    temp_tag_list=[]\n    with open(self.data_file) as input:\n      for row in csv.reader(input,delimiter=\"\\t\"):\n          if 'id' not in row :\n            if (row==[]) or (row==['', '']):\n              wordTag['word']=temp_word_list\n              wordTag['tag']=temp_tag_list\n              final_list.append(wordTag)\n              wordTag={}\n              temp_word_list=[]\n              temp_tag_list=[]\n              continue\n            temp_word_list.append(row[0])\n            temp_tag_list.append(row[1])\n    return final_list\n\n  def __len__(self) -> int:\n        return len(self.data)\n\n  def __getitem__(self, index) -> List[Dict]:\n        return self.data[index]\n    \n\n      ","metadata":{"id":"aXwgarl7QKs2","execution":{"iopub.status.busy":"2022-04-19T20:15:34.697176Z","iopub.execute_input":"2022-04-19T20:15:34.697419Z","iopub.status.idle":"2022-04-19T20:15:34.709175Z","shell.execute_reply.started":"2022-04-19T20:15:34.697387Z","shell.execute_reply":"2022-04-19T20:15:34.707631Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"vocab,embeddings = [],[]\nwith open('../input/glove-embeddings/glove.6B.300d.txt','rt') as fi:\n    full_content = fi.read().strip().split('\\n')\nfor i in range(len(full_content)):\n    i_word = full_content[i].split(' ')[0]\n    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n    vocab.append(i_word)\n    embeddings.append(i_embeddings)\n\nimport numpy as np\nvocab_npa = np.array(vocab)\nembs_npa = np.array(embeddings)\n\n#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\nvocab_npa = np.insert(vocab_npa, 0, '<pad>')\nvocab_npa = np.insert(vocab_npa, 1, '<unk>')\nprint(vocab_npa[:10])\n\npad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\nunk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n\n\n\n#insert embeddings for pad and unk tokens at top of embs_npa.\nembs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\nprint(embs_npa.shape)\n\nword2ind={k: v for v, k in enumerate(vocab_npa)}\n\ndel vocab,embeddings","metadata":{"id":"maI-tuANTDxy","execution":{"iopub.status.busy":"2022-04-19T20:15:34.710321Z","iopub.execute_input":"2022-04-19T20:15:34.710945Z","iopub.status.idle":"2022-04-19T20:16:48.517756Z","shell.execute_reply.started":"2022-04-19T20:15:34.710910Z","shell.execute_reply":"2022-04-19T20:16:48.516960Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"input_file=\"../input/ner-dataset/train_p.tsv\"\ntrain_data=NerDataset(input_file,device=\"cuda\")\ninput_file=\"../input/ner-dataset/dev_p.tsv\"\nval_data=NerDataset(input_file,device=\"cuda\")","metadata":{"id":"7_hj2pHiHF2A","execution":{"iopub.status.busy":"2022-04-19T20:16:48.519795Z","iopub.execute_input":"2022-04-19T20:16:48.520056Z","iopub.status.idle":"2022-04-19T20:16:48.802441Z","shell.execute_reply.started":"2022-04-19T20:16:48.520022Z","shell.execute_reply":"2022-04-19T20:16:48.801647Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Use this code when not using pre-trained embedding to create lookup index\n# def build_vocab(dataset, min_freq=1):\n#     all_words = [item for sublist in dataset for item in sublist['word']]\n#     # [word for sample in dataset for word in sample[\"word\"]]\n#     counter = Counter(all_words)\n#     # we add special tokens for handling padding and unknown words at testing time.\n#     return Vocab(counter, min_freq=min_freq,specials=['<pad>', '<unk>'])\n# word2ind = build_vocab(train_data,min_freq=5)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T20:16:48.815470Z","iopub.execute_input":"2022-04-19T20:16:48.815733Z","iopub.status.idle":"2022-04-19T20:16:48.821553Z","shell.execute_reply.started":"2022-04-19T20:16:48.815700Z","shell.execute_reply":"2022-04-19T20:16:48.820871Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_vocab_tag(dataset):\n    all_words = [item for sublist in dataset for item in sublist['tag']]\n    counter = Counter(all_words)\n    # we add special tokens for handling padding and unknown words at testing time.\n    return Vocab(counter,specials=['<pad>'])\nvocabulary_tag = build_vocab_tag(train_data)","metadata":{"id":"eBpj8PYg1E7z","execution":{"iopub.status.busy":"2022-04-19T20:16:48.822847Z","iopub.execute_input":"2022-04-19T20:16:48.823186Z","iopub.status.idle":"2022-04-19T20:16:48.856062Z","shell.execute_reply.started":"2022-04-19T20:16:48.823150Z","shell.execute_reply":"2022-04-19T20:16:48.855355Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# partial copy of code from notebook from classes\ndef prepare_batch(batch: List[Dict]) -> List[Dict]:\n  PAD_TOKEN = \"<pad>\"\n  UNK_TOKEN = \"<unk>\"\n  # extract features and labels from batch\n  x = [sample[\"word\"] for sample in batch]\n  y = [sample[\"tag\"] for sample in batch]\n  # convert words to index\n  x = [[word2ind.get(word, word2ind[UNK_TOKEN]) for word in sample] for sample in x]\n  # convert labels to index\n  y = [[vocabulary_tag.stoi.get(label) for label in sample] for sample in y]\n  # convert features to tensor and pad them\n  x = pad_sequence(\n    [torch.as_tensor(sample) for sample in x],\n    padding_value=word2ind['<pad>']\n  )\n  # convert and pad labels too\n  y = pad_sequence(\n    [torch.as_tensor(sample) for sample in y],\n    padding_value=vocabulary_tag.stoi['<pad>']\n  )\n  return {\"word\": x, \"tag\": y}","metadata":{"id":"xj4VpaPwHXG7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loader parameters\ncollate_fn = prepare_batch # the function that will prepare the data for the model\nbatch_sizes = 32\n# num_workers = min(os.cpu_count(), 4)  # it is usually 4 workers per GPU\nis_train_dataloader = True # we don\"t want to shuffle dev and test data\ntrain_data_loader = DataLoader(\n  train_data,\n  collate_fn=collate_fn,\n  shuffle=True,\n  batch_size=batch_sizes\n)\nval_data_loader=DataLoader(\n  val_data,\n  collate_fn=collate_fn,\n  shuffle=False,\n  batch_size=batch_sizes\n)","metadata":{"id":"OUVOJFmsHnSI","execution":{"iopub.status.busy":"2022-04-19T20:17:13.751116Z","iopub.execute_input":"2022-04-19T20:17:13.751713Z","iopub.status.idle":"2022-04-19T20:17:13.758627Z","shell.execute_reply.started":"2022-04-19T20:17:13.751656Z","shell.execute_reply":"2022-04-19T20:17:13.757978Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class HParams():\n    vocab_size = len(word2ind)\n    hidden_dim = 264\n    embedding_dim = 300\n    num_classes = len(vocabulary_tag) # number of different universal POS tags\n    bidirectional = True\n    num_layers = 2\n    emb_dropout=0.6\n    lstm_dropout=0.5\n    fc_dropout=0.6\nparams = HParams()","metadata":{"id":"iDLGudOPpMHP","execution":{"iopub.status.busy":"2022-04-19T20:17:13.760494Z","iopub.execute_input":"2022-04-19T20:17:13.760931Z","iopub.status.idle":"2022-04-19T20:17:13.770930Z","shell.execute_reply.started":"2022-04-19T20:17:13.760890Z","shell.execute_reply":"2022-04-19T20:17:13.770301Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class NERModel(nn.Module):\n    # we provide the hyperparameters as input\n    def __init__(self, hparams):\n        super().__init__()\n        pprint(vars(hparams))\n#         self.word_embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n        self.word_embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(),freeze=False)\n        self.emb_dropout = nn.Dropout(hparams.emb_dropout)\n        self.lstm = nn.LSTM(input_size=hparams.embedding_dim, \n                            hidden_size=hparams.hidden_dim, \n                            bidirectional=hparams.bidirectional,\n                            num_layers=hparams.num_layers, \n                            dropout = hparams.lstm_dropout)\n        lstm_output_dim = hparams.hidden_dim if hparams.bidirectional is False else hparams.hidden_dim * 2\n        self.fc_dropout = nn.Dropout(hparams.fc_dropout)\n        self.classifier = nn.Linear(lstm_output_dim, hparams.num_classes)\n\n    \n    def forward(self, x):\n        embeddings =  self.emb_dropout(self.word_embedding(x))\n        o, (h, c) = self.lstm(embeddings)\n        output = self.classifier(self.fc_dropout(o))\n        return output\n","metadata":{"id":"jolQJyeht-Pz","execution":{"iopub.status.busy":"2022-04-19T20:17:13.772724Z","iopub.execute_input":"2022-04-19T20:17:13.773244Z","iopub.status.idle":"2022-04-19T20:17:13.784221Z","shell.execute_reply.started":"2022-04-19T20:17:13.773208Z","shell.execute_reply":"2022-04-19T20:17:13.783397Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class Trainer():\n    \"\"\"Utility class to train and evaluate a model.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        loss_function,\n        optimizer,\n        label_vocab: Vocab,\n        log_steps:int=10_000,\n        log_level:int=2):\n        \"\"\"\n        Args:\n            model: the model we want to train.\n            loss_function: the loss_function to minimize.\n            optimizer: the optimizer used to minimize the loss_function.\n        \"\"\"\n        self.model = model\n        self.loss_function = loss_function\n        self.optimizer = optimizer\n\n        self.label_vocab = label_vocab\n        self.log_steps = log_steps\n        self.log_level = log_level\n\n    def train(self, train_dataset:Dataset, \n              valid_dataset:Dataset, \n              epochs:int=1):\n        \"\"\"\n        Args:\n            train_dataset: a Dataset or DatasetLoader instance containing\n                the training instances.\n            valid_dataset: a Dataset or DatasetLoader instance used to evaluate\n                learning progress.\n            epochs: the number of times to iterate over train_dataset.\n\n        Returns:\n            avg_train_loss: the average training loss on train_dataset over\n                epochs.\n        \"\"\"\n        assert epochs > 1 and isinstance(epochs, int)\n        if self.log_level > 0:\n            print('---------------Training  Started-----------')\n        train_loss = 0.0\n        plot_loss_train=[]\n        plot_loss_val=[]\n        for epoch in range(epochs):\n            if self.log_level > 0:\n                print(' Epoch {:03d}'.format(epoch + 1))\n\n            epoch_loss = 0.0\n            self.model.train()\n\n            # for each batch \n            for step, sample in enumerate(tqdm(train_dataset)):\n                inputs = sample['word'].to(device)\n                labels = sample['tag'].to(device)\n                self.optimizer.zero_grad()\n                predictions = self.model(inputs)\n                predictions = predictions.view(-1, predictions.shape[-1])\n                labels = labels.view(-1)\n                sample_loss = self.loss_function(predictions, labels)\n                sample_loss.backward()\n                self.optimizer.step()\n\n                epoch_loss += sample_loss.tolist()\n\n                if self.log_level > 1 and step % self.log_steps == self.log_steps - 1:\n                    print('[Epoch: {:2d} @ step {}] current avg loss = {:0.4f} '.format(epoch, step, epoch_loss / (step + 1)))\n            avg_epoch_loss = epoch_loss / len(train_dataset)\n            train_loss += avg_epoch_loss\n            if self.log_level > 0:\n                print('\\t[Epoch: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n                plot_loss_train.append(avg_epoch_loss)\n\n            valid_loss = self.evaluate(valid_dataset)\n            if self.log_level > 0:\n                print('\\t[Epoch: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n                plot_loss_val.append(valid_loss)\n\n        if self.log_level > 0:\n            print('... Done!')\n        \n        avg_epoch_loss = train_loss / epochs\n        return self.model,plot_loss_train,plot_loss_val\n    \n\n    def evaluate(self, valid_dataset):\n        \"\"\"\n        Args:\n            valid_dataset: the dataset to use to evaluate the model.\n\n        Returns:\n            avg_valid_loss: the average validation loss over valid_dataset.\n        \"\"\"\n        valid_loss = 0.0\n        # set dropout to 0!! Needed when we are in inference mode.\n        self.model.eval()\n        with torch.no_grad():\n            for sample in valid_dataset:\n                inputs = sample['word'].to(device)\n                labels = sample['tag'].to(device)\n                predictions = self.model(inputs)\n                predictions = predictions.view(-1, predictions.shape[-1])\n                labels = labels.view(-1)\n                sample_loss = self.loss_function(predictions, labels)\n                valid_loss += sample_loss.tolist()\n        \n        return valid_loss / len(valid_dataset)\n\n\n\n    def predict(self, x):\n        \"\"\"\n        Args:\n            x: a tensor of indices.\n        Returns: \n            A list containing the predicted POS tag for each token in the\n            input sentences.\n        \"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            logits = self.model(x)\n            predictions = torch.argmax(logits, -1)\n            return predictions\n\n    \n\n    \n    \n","metadata":{"id":"2H7fsloct-Tp","execution":{"iopub.status.busy":"2022-04-19T20:17:13.872123Z","iopub.execute_input":"2022-04-19T20:17:13.872433Z","iopub.status.idle":"2022-04-19T20:17:13.894511Z","shell.execute_reply.started":"2022-04-19T20:17:13.872401Z","shell.execute_reply":"2022-04-19T20:17:13.893875Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tagger = NERModel(params).to(device)","metadata":{"id":"RG7r41zbt-XA","outputId":"cf481d2d-cc0f-4c9a-9dd5-cf38ec0297dc","execution":{"iopub.status.busy":"2022-04-19T20:17:13.917642Z","iopub.execute_input":"2022-04-19T20:17:13.918203Z","iopub.status.idle":"2022-04-19T20:17:17.341358Z","shell.execute_reply.started":"2022-04-19T20:17:13.918167Z","shell.execute_reply":"2022-04-19T20:17:17.340608Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model = tagger,\n    loss_function = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>']),\n    optimizer = optim.Adam(tagger.parameters()),\n    label_vocab=vocabulary_tag.stoi)","metadata":{"id":"6s1eocPStAAH","execution":{"iopub.status.busy":"2022-04-19T20:17:17.343613Z","iopub.execute_input":"2022-04-19T20:17:17.343878Z","iopub.status.idle":"2022-04-19T20:17:17.348772Z","shell.execute_reply.started":"2022-04-19T20:17:17.343850Z","shell.execute_reply":"2022-04-19T20:17:17.348087Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#get trained model along with train loss and val loss arrays\ntrained_model,t_l,v_l=trainer.train(train_data_loader, val_data_loader,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the loss curves\nplt.figure(figsize=[8,6])\nplt.plot(t_l,'r',linewidth=3.0)\nplt.plot(v_l,'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves With Glove Embeddings',fontsize=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#partial copy of code from notebook from class\ndef to_labels(xx:list):\n  return [[list(vocabulary_tag.stoi.keys())[list(vocabulary_tag.stoi.values()).index(w)] for w in xx]]\ndef compute_precision(model, dataset):\n    all_predictions = list()\n    all_labels = list()\n    for sample in dataset:\n        inputs = sample['word'].to('cuda')\n        print(inputs)\n        labels = sample['tag'].to('cuda')\n        predictions = model(inputs)\n        predictions = torch.argmax(predictions, -1)\n        print(predictions)\n        labels = labels\n        predictions = predictions.view(-1)\n        labels = labels.view(-1)\n        valid_indices = labels != 0\n        valid_predictions = predictions[valid_indices]\n        valid_labels = labels[valid_indices]\n        all_predictions.extend(valid_predictions.tolist())\n        all_labels.extend(valid_labels.tolist())\n    precision = f1_score(all_labels,all_predictions, average=None, zero_division=0)\n    seqeval = f1(to_labels(all_labels),to_labels(all_predictions),mode='strict',)\n    cm=confusion_matrix(all_labels,all_predictions,normalize='pred')\n    \n    return precision,seqeval,cm","metadata":{"execution":{"iopub.status.busy":"2022-04-19T20:26:39.292753Z","iopub.execute_input":"2022-04-19T20:26:39.293613Z","iopub.status.idle":"2022-04-19T20:26:39.304253Z","shell.execute_reply.started":"2022-04-19T20:26:39.293480Z","shell.execute_reply":"2022-04-19T20:26:39.303537Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"pre,seqP,cm=compute_precision(trained_model,val_data_loader)","metadata":{"id":"-lUNWFte60Pc","outputId":"a312d2f6-aee6-44fb-f6e3-00e1cd0eeefc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8)) \nplt.title('F1 scores of each Class with Glove Embeddings',fontsize=16)\nsns.barplot(vocabulary_tag.itos[1:],pre.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T20:24:05.592675Z","iopub.status.idle":"2022-04-19T20:24:05.593293Z","shell.execute_reply.started":"2022-04-19T20:24:05.593060Z","shell.execute_reply":"2022-04-19T20:24:05.593084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8)) \nax= plt.subplot()\nsns.heatmap(cm, fmt='g',cmap=\"OrRd\")\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title(' Normalized Confusion Matrix with Glove Embeddings'); \nax.xaxis.set_ticklabels(vocabulary_tag.itos[1:]); ax.yaxis.set_ticklabels(vocabulary_tag.itos[1:],rotation=0);\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T20:24:05.597875Z","iopub.status.idle":"2022-04-19T20:24:05.598449Z","shell.execute_reply.started":"2022-04-19T20:24:05.598216Z","shell.execute_reply":"2022-04-19T20:24:05.598240Z"},"trusted":true},"execution_count":null,"outputs":[]}]}