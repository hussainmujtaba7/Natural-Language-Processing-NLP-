{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom zipfile import ZipFile\nfrom sklearn.model_selection import train_test_split\nfrom typing import Dict\nimport torch\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom operator import itemgetter\nfrom sklearn.metrics import precision_score\nimport pickle\nimport time\nfrom tqdm import tqdm\n\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformer_model = \"SpanBERT/spanbert-large-cased\"\ntransformer_model = 'bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(transformer_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/nlphw3/dev.tsv',sep='\\t')\ndata=pd.read_csv('/kaggle/input/nlphw3/train.tsv',sep='\\t',header=None)\ndata.columns=test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = train_test_split(data, test_size=0.2,random_state=4)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CorefData(torch.utils.data.Dataset):\n    def __init__(self, data):\n        print(data.shape)\n        self.lemmas = data[\"Text\"]\n        self.label = data[[\"A-coref\", \"B-coref\"]].apply(\n            lambda x: 0 if x[\"A-coref\"] \n            else 1 if x[\"B-coref\"] else 2, axis=1)\n        self.pronoun_offset=data['Pronoun-offset']\n        self.A_offset=data['A-offset']\n        self.B_offset=data['B-offset']\n        \n        \n        \n    def __len__(self):\n        return len(self.lemmas)\n  \n    def __getitem__(self,idx):\n        return self.lemmas[idx],self.label[idx],self.pronoun_offset[idx],self.A_offset[idx],self.B_offset[idx]\n    \n    \n    \n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_index(offsets, offset_list):\n# Get the rows where both elements are non-zero and set them to -1\n    zeros = (offsets[:, :, 0] == 0) & (offsets[:, :, 1] == 0)\n    offsets[zeros] = -1\n# Use boolean indexing to remove rows where both elements are zero\n    word_indexes = []\n    for i,offset in enumerate(offset_list):\n        condition = (offsets[i,:, 0] == offset)\n        word_index = torch.nonzero(condition)\n        word_indexes.append(word_index)\n    word_index_tensor=torch.tensor(word_indexes).squeeze(0)\n    return word_index_tensor\n\n\ndef collate_fn(batch):\n    batch_out = tokenizer(\n        [sentence[0] for sentence in batch],\n        return_tensors=\"pt\",\n        padding=True,\n        is_split_into_words=False,\n        return_offsets_mapping=True\n    ) \n    offset_mapping=batch_out['offset_mapping']\n    batch_out['label']=[sentence[1] for sentence in batch]\n    batch_out['Pronoun_loc']=get_word_index(offset_mapping,[sentence[2] for sentence in batch])\n    batch_out['A_loc']=get_word_index(offset_mapping,[sentence[3] for sentence in batch])\n    batch_out['B_loc']=get_word_index(offset_mapping,[sentence[4] for sentence in batch])\n    return batch_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset=CorefData(train)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=collate_fn)\nval_dataset=CorefData(val)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False,collate_fn=collate_fn)\ntest_dataset=CorefData(test)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False,collate_fn=collate_fn)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in val_dataloader:\n    print(i)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Coref(torch.nn.Module):\n    def __init__(self, pre_trained_transformer_model):\n        super(Coref, self).__init__()\n        self.transformer_model = pre_trained_transformer_model\n        for param in self.transformer_model.parameters():\n            param.requires_grad = False\n        self.dropout = torch.nn.Dropout(0.5)\n        self.lstm = torch.nn.LSTM(input_size=self.transformer_model.config.hidden_size, \n                            hidden_size=786, \n                            batch_first=True,\n                            bidirectional=True,\n                            num_layers=2,\n                            dropout=0.5\n                                 )\n        self.relu = torch.nn.ReLU()\n        self.fc1 = torch.nn.Linear((786 * 2)*3, 1024)\n        self.fc2 = torch.nn.Linear(1024, 3)\n        \n    def forward(self, batch):\n        # Get the last hidden state of the BERT model\n        input_ids = batch['input_ids'].to(device)\n        attention_mask=batch['attention_mask'].to(device)\n        transformers_outputs = self.transformer_model(input_ids,attention_mask)\n        embed_out = torch.stack(transformers_outputs.hidden_states[-4:], dim=0).sum(dim=0)\n        \n        output,_ = self.lstm(embed_out)\n        out_sent = torch.mean(output, dim=1)\n        \n        out_pron = output[torch.arange(output.shape[0]), batch['Pronoun_loc'], :]\n        out_A = output[torch.arange(output.shape[0]), batch['A_loc'], :]\n        out_B = output[torch.arange(output.shape[0]), batch['B_loc'], :]\n        \n        \n        out_pron_A = out_pron-out_A/2\n        out_pron_B = out_pron-out_B/2\n\n        total_out= torch.cat((out_sent,out_pron_A,out_pron_B),1)\n        relu1 = self.relu(total_out)\n        dense1 = self.fc1(self.dropout(relu1))\n        relu2 = self.relu(dense1)\n        preds = self.fc2(self.dropout(relu2))\n        return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_trained_transformer_model=AutoModel.from_pretrained(transformer_model, output_hidden_states=True)\nmodel=Coref(pre_trained_transformer_model).to(device)\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in tqdm(iterator):\n        \n        text = batch\n        tags = torch.tensor(batch['label'])\n        optimizer.zero_grad()\n        predictions = model(text)\n        predictions = predictions.view(-1, predictions.shape[-1])\n        tags = tags.view(-1).type(torch.LongTensor).to(device)\n        loss = criterion(predictions, tags)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            text = batch\n            tags = torch.tensor(batch['label'])\n            \n            predictions = model(text)\n            \n            predictions = predictions.view(-1, predictions.shape[-1])\n            tags = tags.view(-1).type(torch.LongTensor).to(device)\n            \n\n            \n            loss = criterion(predictions, tags)\n            \n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n\ndef predict(model, iterator):\n    pred=[]\n    tag=[]\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n\n            text = batch\n            tags = torch.tensor(batch['label'])\n            \n            predictions = model(text)\n            \n            predictions = predictions.view(-1, predictions.shape[-1])\n            max_preds = predictions.argmax(dim = 1, keepdim = False)\n            tags = tags.view(-1)\n\n            pred.append(max_preds.tolist())\n            tag.append(tags.tolist())\n            \n        \n    return pred,tag\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss= train_model(model, train_dataloader, optimizer, criterion)\n    valid_loss= evaluate(model, val_dataloader, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut1-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred,label=predict(model, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = [item for sublist in pred for item in sublist]\nlabel = [item for sublist in label for item in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(pred,label,average=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model-hw2.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config0: add all emb\n# config1:add emb/2\n# config2:sub emb/2 array([0.77453581, 0.79146919, 0.55045872])\n# config3: consine  array([0.78947368, 0.81339713, 0.56363636])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spanbert# array([0.78074866, 0.79816514, 0.53061224])\n# bert # array([0.77804296, 0.78010471, 0.56074766])\n# array([0.81909548, 0.78061224, 0.59322034])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}